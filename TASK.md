# Техническое задание: Миграция Backend на Python (aiohttp)

## 1. Обзор задачи
Необходимо перенести серверную логику проекта **Open Scouts** с Supabase Edge Functions (Deno) на собственный Python-бэкенд.
Текущий проект представляет собой систему мониторинга, использующую AI для поиска информации в интернете по расписанию.

## 2. Ключевые требования к стеку
*   **Фреймворк**: Использовать **только `aiohttp`**.
    *   ❌ ЗАПРЕЩЕНО: FastAPI, Flask, Django.
    *   ❌ ЗАПРЕЩЕНО: Pydantic (валидацию делать нативными средствами Python или простейшими схемами).
*   **База данных**:
    *   Поддержка **PostgreSQL** (как основная БД).
    *   Возможность переключения на **SQLite** (для локальной разработки/тестов).
    *   Использовать `SQLAlchemy Core` (или сырой SQL через `aiosqlite`/`asyncpg`). ORM использовать с осторожностью или избегать.
*   **AI (LLM)**: Переход с OpenAI API на локальный **Ollama**.
*   **Скрапинг**: Переход с Firecrawl на **Crawl4AI**.
*   **Инфраструктура**: Docker Compose (приложение + БД + Ollama + ChromaDB/PgVector).

## 3. Архитектура системы
Система должна состоять из следующих компонентов, запускаемых через `docker-compose`:
1.  **Frontend**: Существующий Next.js проект (требует минимальных правок ENV-переменных).
2.  **Backend (Python)**:
    *   API сервер (aiohttp).
    *   Планировщик задач (аналог `pg_cron` - можно использовать `apscheduler` или простой async loop).
    *   Воркеры для выполнения скаутов (изолированные процессы или корутины).
3.  **Database**: PostgreSQL (или SQLite файл).
4.  **AI Service**: Ollama контейнер.

## 4. Функциональные блоки для реализации

### 4.1. База данных и Модели
Необходимо портировать текущую схему из `supabase/migrations/schema.sql`.
*   Таблицы: `scouts`, `scout_executions`, `scout_messages`, `user_preferences`.
*   **Важно**: Реализовать абстракцию для работы с БД, чтобы прозрачно переключаться между PG и SQLite.

### 4.2. API Сервер (aiohttp)
Реализовать REST API, повторяющий логику Supabase Client (или создать новые эндпоинты, на которые переключить фронт).
*   `POST /auth/login`, `/auth/signup` (JWT токены).
*   `GET /scouts`, `POST /scouts` (CRUD операции).
*   `POST /api/chat` (Чат-бот для настройки скаутов, стриминг ответа).

### 4.3. Агент Скаута (Core Logic)
Переписать логику из `supabase/functions/scout-cron/index.ts` на Python.
*   **Цикл агента**:
    1.  Получить задачу (Scout).
    2.  Сгенерировать план поиска (через Ollama).
    3.  Выполнить поиск и скрапинг (через Crawl4AI).
    4.  Проанализировать результаты.
    5.  Сгенерировать саммари.
    6.  Сохранить результат и эмбеддинг.
*   **Инструменты**:
    *   `search_web`: Поиск (можно использовать DuckDuckGo Search API или Crawl4AI search capabilities).
    *   `scrape_website`: Crawl4AI для получения Markdown контента.

### 4.4. Планировщик (Scheduler)
*   Реализовать сервис, который проверяет таблицу `scouts` раз в минуту.
*   Выбирает скауты, у которых `next_run_at <= now()`.
*   Запускает выполнение агента в фоне.

## 5. Этапы сдачи
1.  Рабочий `docker-compose.yml`.
2.  Backend на `aiohttp` с подключением к БД.
3.  Реализованный класс `ScoutAgent` с интеграцией Ollama и Crawl4AI.
4.  Инструкция по запуску.
